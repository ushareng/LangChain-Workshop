{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7mvtfw1FPvz",
        "outputId": "015416b0-3fb3-4ccc-ce4e-4aa1827c6b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain tiktoken keras-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smw9RQDEFgsq",
        "outputId": "c27c5304-b05f-4f40-8bef-897f1f6eb4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/vocab.json\n",
            "1042301/1042301 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/merges.txt\n",
            "456318/456318 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/model.h5\n",
            "497986112/497986112 [==============================] - 13s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 768) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "gpt2_llm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_extra_large_en\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzZSMl1VUyqV",
        "outputId": "e6ede1e0-e299-4b0a-d6e8-405a7a6715d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_extra_large_en/v1/model.h5\n",
            "6231301960/6231301960 [==============================] - 75s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 1600) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n"
      ],
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter()"
      ],
      "metadata": {
        "id": "pyPulL7tvQOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the doc\n",
        "with open('/content/state_of_the_union.txt') as f:\n",
        "    how_to_win_friends = f.read()\n",
        "texts = text_splitter.split_text(how_to_win_friends)"
      ],
      "metadata": {
        "id": "sCfCSX9sOv2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6KdLobvkLje",
        "outputId": "23ec14fc-57eb-47a5-f4c5-9523dc15ad6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts[:3]]"
      ],
      "metadata": {
        "id": "8L6ztJUJO2j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "WlSA_nIxdrSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "lFYz2IztO2nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfo2cX1cJyTM",
        "outputId": "369a1698-bb89-45b9-e8bb-7f434110317b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "\n",
        "# set context window size\n",
        "context_window = 2048\n",
        "# set number of output tokens\n",
        "num_output = 256\n",
        "\n",
        "# store the pipeline/model outisde of the LLM class to avoid memory issues\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        prompt_length = len(prompt)\n",
        "        response = gpt2_llm.generate(prompt)\n",
        "\n",
        "        # only return newly generated tokens\n",
        "        return response[prompt_length:]\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"name_of_model\": model_name}\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n"
      ],
      "metadata": {
        "id": "KemhGFnMJsKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(CustomLLM(),\n",
        "                             chain_type=\"map_reduce\")\n",
        "\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4nRP8oGO2qf",
        "outputId": "dec8860a-fc4a-4b4c-d9b4-624c95846be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1.) The United States and Europe are imposing sanctions on Russian officials and companies, and\n",
            "the Russian Central Bank is restricting the access of its banks to the international financial\n",
            "system.   2.) Russia's economy has been hit hard by sanctions. The ruble has dropped 40%, inflation\n",
            "is running at 15%, and Russia has lost its place as the world's top energy producer.   3.) Putin's\n",
            "response has been to escalate the military conflict in Ukraine, and he has sent troops into Crimea\n",
            "and Eastern Ukraine.   4.) Russia's economy is in a deep depression. The ruble has dropped,\n",
            "inflation has skyrocketed, and Russia's GDP has been cut in half.   5.) The Russian military is\n",
            "invading Eastern Ukraine.   6.) Putin is a Russian dictator who wants to conquer the Ukraine.   7.)\n",
            "Putin is a dictator who is occupying Ukraine.  8.) Putin is a tyrant.   9.) Putin is a tyrant.\n",
            "Putin is a dictator.  He is invading Ukraine.  He is occupying Ukraine.  He is destroying Ukraine.\n",
            "Putin is destroying Europe.  Putin is destroying the free world.   He is a tyrant.   10.) Putin is a\n",
            "tyrant.   11.) He is a tyrant.   12.) He is a tyrant.   13.) He is a tyrant.   14.) Putin is a\n",
            "tyrant.  15.) He is a tyrant.   16.) He is a tyrant.  17.) He is a tyrant.  18.) He is a tyrant.\n",
            "19.) He is a tyrant.  20.) He is a tyrant.  The President's plan was to \"\" \"\" \"\"Buy American\"\"\n",
            "American products.\"He also used the phrase \"Buy America\"\" and \"Buy American\" to promote the American\n",
            "economy, but his plan was to \" \"\" \"\" \"\" \"\" \" \"\" \" \" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojL-PxzpZZcZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}